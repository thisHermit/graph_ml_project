{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "627dc7ea-faa3-48b0-a610-25caa31ba888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from numpy.linalg import eigh\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "# find device\n",
    "if torch.cuda.is_available(): # NVIDIA\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available(): # apple M1/M2\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb9fdbc-60c8-45e2-aade-8c79ded36ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10\n",
      "Label distribution: [0.08884393 0.03540881 0.06419571 0.06226432 0.6272418  0.19755358\n",
      " 0.10687023 0.18412581 0.01995769 0.2598179 ]\n"
     ]
    }
   ],
   "source": [
    "dataset = pyg.datasets.LRGBDataset(root='dataset/peptides-func', name=\"Peptides-func\")\n",
    "\n",
    "# Check if dataset has splits; if not, create them manually\n",
    "if hasattr(dataset, 'train_val_test_idx'):\n",
    "    peptides_train = dataset[dataset.train_val_test_idx['train']]\n",
    "    peptides_val = dataset[dataset.train_val_test_idx['val']]\n",
    "    peptides_test = dataset[dataset.train_val_test_idx['test']]\n",
    "else:\n",
    "    # Create train, val, test splits manually\n",
    "    num_train = int(0.8 * len(dataset))\n",
    "    num_val = int(0.1 * len(dataset))\n",
    "    num_test = len(dataset) - num_train - num_val\n",
    "    peptides_train, peptides_val, peptides_test = torch.utils.data.random_split(dataset, [num_train, num_val, num_test])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = pyg.loader.DataLoader(peptides_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = pyg.loader.DataLoader(peptides_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = pyg.loader.DataLoader(peptides_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check number of classes and label distribution\n",
    "if hasattr(dataset, 'num_tasks'):\n",
    "    num_classes = dataset.num_tasks\n",
    "elif hasattr(dataset, 'num_classes'):\n",
    "    num_classes = dataset.num_classes\n",
    "else:\n",
    "    # Assume binary classification if not specified\n",
    "    num_classes = 1\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "all_labels = np.concatenate([data.y.numpy() for data in dataset], axis=0)\n",
    "label_distribution = np.mean(all_labels, axis=0)\n",
    "print(f\"Label distribution: {label_distribution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85632547-aa9d-4e91-b4e7-5839318ad1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_laplace_pe(data, num_eigenvec=10):\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    A = nx.adjacency_matrix(G).astype(float)\n",
    "    num_nodes = A.shape[0]\n",
    "    D = np.diag(np.array(A.sum(axis=1)).flatten())\n",
    "    L = D - A.todense()\n",
    "    L = torch.tensor(L, dtype=torch.float, device=device)\n",
    "    try:\n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(L)\n",
    "    except RuntimeError:\n",
    "        eigenvalues, eigenvectors = torch.symeig(L, eigenvectors=True)\n",
    "    available_eigenvec = eigenvectors.shape[1] - 1\n",
    "    actual_num_eigenvec = min(num_eigenvec, available_eigenvec)\n",
    "    eigenvectors = eigenvectors[:, 1:1 + actual_num_eigenvec]\n",
    "    if actual_num_eigenvec < num_eigenvec:\n",
    "        pad_size = num_eigenvec - actual_num_eigenvec\n",
    "        padding = torch.zeros(eigenvectors.shape[0], pad_size, device=device)\n",
    "        eigenvectors = torch.cat([eigenvectors, padding], dim=1)\n",
    "    return eigenvectors  # Shape: (num_nodes, num_eigenvec)\n",
    "\n",
    "# Random Walk Structural Embeddings (RWSE)\n",
    "def compute_rwse(data, walk_length=10):\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    A = nx.adjacency_matrix(G).astype(float)\n",
    "    A = A.todense()\n",
    "    num_nodes = A.shape[0]\n",
    "    A = torch.tensor(A, dtype=torch.float, device=device)\n",
    "    rw_features = []\n",
    "    A_power = A.clone()\n",
    "    for _ in range(walk_length):\n",
    "        diag = torch.diagonal(A_power)\n",
    "        rw_features.append(diag)\n",
    "        A_power = torch.matmul(A_power, A)\n",
    "    rwse = torch.stack(rw_features, dim=1)  # (num_nodes, walk_length)\n",
    "    return rwse  # Shape: (num_nodes, walk_length)\n",
    "\n",
    "# SignNet to ensure sign invariance\n",
    "class SignNet(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SignNet, self).__init__()\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.phi(x) + self.phi(-x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f01335-eb42-44b0-a55a-6eb74485b181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07753953-23ee-4ab4-a348-d67524a13822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b99db91a-a156-4720-8982-f76ce006fda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes (tasks): 10\n",
      "Label distribution: [0.08884393 0.03540881 0.06419571 0.06226432 0.6272418  0.19755358\n",
      " 0.10687023 0.18412581 0.01995769 0.2598179 ]\n",
      "Number of node features: 9\n",
      "Epoch [1/30] Train Loss: 0.3816, Val Loss: 0.3673\n",
      "Epoch [2/30] Train Loss: 0.3627, Val Loss: 0.3721\n",
      "Epoch [3/30] Train Loss: 0.3576, Val Loss: 0.3864\n",
      "Epoch [4/30] Train Loss: 0.3550, Val Loss: 0.3862\n",
      "Epoch [5/30] Train Loss: 0.3526, Val Loss: 0.3811\n",
      "Epoch [6/30] Train Loss: 0.3507, Val Loss: 0.3777\n",
      "Epoch [7/30] Train Loss: 0.3482, Val Loss: 0.3639\n",
      "Epoch [8/30] Train Loss: 0.3471, Val Loss: 0.3665\n",
      "Epoch [9/30] Train Loss: 0.3448, Val Loss: 0.3673\n",
      "Epoch [10/30] Train Loss: 0.3432, Val Loss: 0.3786\n",
      "Epoch [11/30] Train Loss: 0.3416, Val Loss: 0.3574\n",
      "Epoch [12/30] Train Loss: 0.3397, Val Loss: 0.3507\n",
      "Epoch [13/30] Train Loss: 0.3382, Val Loss: 0.3511\n",
      "Epoch [14/30] Train Loss: 0.3375, Val Loss: 0.3511\n",
      "Epoch [15/30] Train Loss: 0.3361, Val Loss: 0.3535\n",
      "Epoch [16/30] Train Loss: 0.3351, Val Loss: 0.3552\n",
      "Epoch [17/30] Train Loss: 0.3331, Val Loss: 0.3478\n",
      "Epoch [18/30] Train Loss: 0.3302, Val Loss: 0.3553\n",
      "Epoch [19/30] Train Loss: 0.3297, Val Loss: 0.3537\n",
      "Epoch [20/30] Train Loss: 0.3286, Val Loss: 0.3587\n",
      "Epoch [21/30] Train Loss: 0.3261, Val Loss: 0.3531\n",
      "Epoch [22/30] Train Loss: 0.3258, Val Loss: 0.3588\n",
      "Epoch [23/30] Train Loss: 0.3234, Val Loss: 0.3546\n",
      "Epoch [24/30] Train Loss: 0.3221, Val Loss: 0.3563\n",
      "Epoch [25/30] Train Loss: 0.3190, Val Loss: 0.3589\n",
      "Epoch [26/30] Train Loss: 0.3183, Val Loss: 0.3510\n",
      "Epoch [27/30] Train Loss: 0.3157, Val Loss: 0.3596\n",
      "Epoch [28/30] Train Loss: 0.3166, Val Loss: 0.3564\n",
      "Epoch [29/30] Train Loss: 0.3149, Val Loss: 0.3563\n",
      "Epoch [30/30] Train Loss: 0.3149, Val Loss: 0.3522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/u12754_3976534/ipykernel_913727/1693741433.py:183: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3483\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv, global_mean_pool\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Define the Graph Transformer Model\n",
    "# ----------------------------\n",
    "class GraphTransformerModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout=0.5):\n",
    "        \"\"\"\n",
    "        A simple 2-layer Transformer-based GNN.\n",
    "\n",
    "        :param in_channels:  Number of input features per node\n",
    "        :param hidden_channels: Hidden layer size\n",
    "        :param out_channels:   Number of prediction classes (or tasks)\n",
    "        :param heads:          Number of attention heads in TransformerConv\n",
    "        :param dropout:        Dropout probability\n",
    "        \"\"\"\n",
    "        super(GraphTransformerModel, self).__init__()\n",
    "        # First transformer layer\n",
    "        self.conv1 = TransformerConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        # After conv1, the output dimension is hidden_channels * heads\n",
    "        self.conv2 = TransformerConv(hidden_channels * heads, hidden_channels, heads=1, dropout=dropout)\n",
    "        \n",
    "        # Final linear layer for graph-level prediction\n",
    "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        :param x:         Node features [num_nodes, in_channels]\n",
    "        :param edge_index: Edge indices [2, num_edges]\n",
    "        :param batch:     Batch indices for each node [num_nodes]\n",
    "        :return:          Model output (logits), shape [batch_size, out_channels]\n",
    "        \"\"\"\n",
    "        # First transformer block\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Second transformer block\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Global average pooling over the nodes in each graph\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Optionally apply dropout before the final linear layer\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Final classification/regression head\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Prepare your dataset & loaders (from your snippet)\n",
    "# ----------------------------\n",
    "import torch_geometric as pyg\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "dataset = pyg.datasets.LRGBDataset(root='dataset/peptides-func', name=\"Peptides-func\")\n",
    "\n",
    "# Check if dataset has splits; if not, create them manually\n",
    "if hasattr(dataset, 'train_val_test_idx'):\n",
    "    peptides_train = dataset[dataset.train_val_test_idx['train']]\n",
    "    peptides_val = dataset[dataset.train_val_test_idx['val']]\n",
    "    peptides_test = dataset[dataset.train_val_test_idx['test']]\n",
    "else:\n",
    "    # Create train, val, test splits manually\n",
    "    num_train = int(0.8 * len(dataset))\n",
    "    num_val = int(0.1 * len(dataset))\n",
    "    num_test = len(dataset) - num_train - num_val\n",
    "    peptides_train, peptides_val, peptides_test = torch.utils.data.random_split(\n",
    "        dataset, [num_train, num_val, num_test]\n",
    "    )\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = pyg.loader.DataLoader(peptides_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = pyg.loader.DataLoader(peptides_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = pyg.loader.DataLoader(peptides_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check number of classes and label distribution\n",
    "if hasattr(dataset, 'num_tasks'):\n",
    "    num_classes = dataset.num_tasks\n",
    "elif hasattr(dataset, 'num_classes'):\n",
    "    num_classes = dataset.num_classes\n",
    "else:\n",
    "    # Assume binary classification if not specified\n",
    "    num_classes = 1\n",
    "print(f\"Number of classes (tasks): {num_classes}\")\n",
    "\n",
    "all_labels = np.concatenate([data.y.numpy() for data in dataset], axis=0)\n",
    "label_distribution = np.mean(all_labels, axis=0)\n",
    "print(f\"Label distribution: {label_distribution}\")\n",
    "\n",
    "# Retrieve the number of node features\n",
    "# (Some PyG datasets use dataset.num_node_features or\n",
    "#  you may inspect the first data.x.size(1) for the correct dimension.)\n",
    "num_node_features = dataset.num_node_features if hasattr(dataset, 'num_node_features') \\\n",
    "                   else peptides_train[0].x.size(-1)\n",
    "print(f\"Number of node features: {num_node_features}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Instantiate the model, optimizer, and loss function\n",
    "# ----------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GraphTransformerModel(\n",
    "    in_channels=num_node_features,\n",
    "    hidden_channels=64,\n",
    "    out_channels=num_classes,  # multi-task or single-task\n",
    "    heads=4,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "# If this is a multi-label classification problem, use BCEWithLogitsLoss\n",
    "# If single-label multi-class, use CrossEntropyLoss\n",
    "# If regression, use MSELoss, etc.\n",
    "# The Peptides-func dataset is typically multi-task classification, so BCEWithLogitsLoss is common:\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Define Training and Evaluation Routines\n",
    "# ----------------------------\n",
    "def train_one_epoch(loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        data.x = data.x.float()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        \n",
    "        # data.y could have shape [batch_size, num_classes] for multi-label\n",
    "        # Make sure to cast to float for BCEWithLogitsLoss\n",
    "        loss = criterion(out, data.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        data.x = data.x.float()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y.float())\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Training Loop\n",
    "# ----------------------------\n",
    "best_val_loss = float('inf')\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_one_epoch(train_loader)\n",
    "    val_loss = evaluate(val_loader)\n",
    "    \n",
    "    # Checkpoint if validation improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    \n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Testing the Best Model\n",
    "# ----------------------------\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "test_loss = evaluate(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc3d79-e6cb-43d5-bb65-8708cd340e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
